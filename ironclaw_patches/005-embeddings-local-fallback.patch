--- Embeddings Local Fallback ---

Description: When using local LLM backends, configure embeddings to also
use local options or disable semantic search gracefully.

File: src/config/embeddings.rs (conceptual patch)

Current behavior:
- Embeddings default to NEAR AI provider
- This requires authentication even for local LLM setups

Proposed change:
- If LLM_BACKEND is local (ollama/openai_compatible), default embeddings to:
  1. Same local endpoint if it supports embeddings
  2. Disable embeddings with a warning
  3. Use a lightweight local embedding model

--- a/src/config/embeddings.rs
+++ b/src/config/embeddings.rs
@@ impl EmbeddingsConfig {
     pub(crate) fn resolve(settings: &Settings, llm_backend: &LlmBackend) -> Self {
         let provider = optional_env("EMBEDDINGS_PROVIDER")
             .unwrap_or(None)
             .or_else(|| settings.embeddings.provider.clone().filter(|s| !s.is_empty()))
-            .unwrap_or_else(|| "nearai".to_string());
+            .unwrap_or_else(|| {
+                // Smart default based on LLM backend
+                match llm_backend {
+                    LlmBackend::Ollama => {
+                        tracing::info!("Using Ollama for embeddings (model: nomic-embed-text)");
+                        "ollama".to_string()
+                    }
+                    LlmBackend::OpenAiCompatible => {
+                        // Check if local endpoint supports embeddings
+                        if let Ok(base_url) = std::env::var("LLM_BASE_URL") {
+                            if base_url.contains("localhost") || base_url.contains("127.0.0.1") {
+                                tracing::info!("Using local endpoint for embeddings");
+                                return "openai_compatible".to_string();
+                            }
+                        }
+                        "nearai".to_string()
+                    }
+                    _ => "nearai".to_string(),
+                }
+            });

---

Note: Ollama supports embeddings via the `ollama pull nomic-embed-text` model.
This allows fully local semantic search without any cloud dependencies.

Trade-offs:
- Requires downloading an additional model for Ollama
- Local embeddings may be lower quality than cloud models
- Memory usage increases with local embedding model
