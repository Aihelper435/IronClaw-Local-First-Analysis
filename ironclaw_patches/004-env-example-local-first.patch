--- Enhanced .env.example with Local-First Guide ---

Description: Add a prominent local-first quick start section at the top
of the example environment file.

File: .env.example

--- a/.env.example
+++ b/.env.example
@@ -1,3 +1,35 @@
+# ═══════════════════════════════════════════════════════════════════════════
+#                         LOCAL-FIRST QUICK START
+# ═══════════════════════════════════════════════════════════════════════════
+#
+# IronClaw works great without any cloud account! Choose one:
+#
+# ── Option 1: Ollama (Recommended for local) ────────────────────────────────
+# Install: https://ollama.com
+# Pull model: ollama pull llama3.2
+# Then uncomment:
+#
+# LLM_BACKEND=ollama
+# OLLAMA_MODEL=llama3.2
+#
+# ── Option 2: LM Studio (Local GUI) ─────────────────────────────────────────
+# Download: https://lmstudio.ai
+# Start the local server, then uncomment:
+#
+# LLM_BACKEND=openai_compatible
+# LLM_BASE_URL=http://localhost:1234/v1
+# LLM_MODEL=your-loaded-model
+#
+# ── Option 3: vLLM / LiteLLM (Self-hosted) ──────────────────────────────────
+# LLM_BACKEND=openai_compatible
+# LLM_BASE_URL=http://your-server:8000/v1
+# LLM_MODEL=meta-llama/Llama-3.1-8B-Instruct
+# LLM_API_KEY=optional-if-required
+#
+# ═══════════════════════════════════════════════════════════════════════════
+#                         CLOUD OPTIONS (below)
+# ═══════════════════════════════════════════════════════════════════════════
+
 # Database Configuration
 DATABASE_URL=postgres://localhost/ironclaw
 DATABASE_POOL_SIZE=10

---

This makes local options immediately visible without scrolling through
cloud configuration details.
